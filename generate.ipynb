{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jw_be\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\jw_be\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\jw_be\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\jw_be\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import heapq\n",
    "import queue\n",
    "from tqdm import tqdm\n",
    "from typing import Callable,List,Tuple,Dict\n",
    "\n",
    "cmap = plt.get_cmap('tab20') # type: ignore\n",
    "State=Dict[any,int] # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G:nx.Graph,state:State=None,label=False):\n",
    "    if state is None:\n",
    "        nx.draw(G, node_size = 80, alpha = 0.8)\n",
    "        plt.show()\n",
    "    else:\n",
    "        map= [v for _,v in state.items()]\n",
    "        u=np.unique(map)\n",
    "        fixed_map=[np.where(u==i)[0][0] for i in map]\n",
    "        node_cmap = [cmap(v) for v in fixed_map]\n",
    "        \n",
    "        pos = nx.spring_layout(G,seed=40)\n",
    "        # add label too nodes\n",
    "        if label:\n",
    "            labels = {node:node for node in G.nodes()}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=9)\n",
    "            \n",
    "        nx.draw(G, pos, node_size = 80, alpha = 0.8, node_color=node_cmap)\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_partitions(state:State)->List[State]:\n",
    "    comm_dict = {}\n",
    "    for node,comm in state.items():\n",
    "        if comm in comm_dict:\n",
    "            comm_dict[comm].add(node)\n",
    "        else:\n",
    "            comm_dict[comm] = {node}\n",
    "    return list(comm_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitions_state(partitions):\n",
    "    state = {}\n",
    "    for i,comm in enumerate(partitions):\n",
    "        for node in comm:\n",
    "            state[node] = i\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_alea(G:nx.Graph,nb_communities=None)->State:\n",
    "    if nb_communities is None: nb_communities = len(G.nodes())\n",
    "    if (nb_communities>len(G.nodes())): raise ValueError(\"nb_communities must be less than the number of nodes\")\n",
    "\n",
    "    state = {node:np.random.randint(0,nb_communities) for node in G.nodes()}\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_greedy(G:nx.Graph)->State:\n",
    "    partitions= nx.algorithms.community.greedy_modularity_communities(G)\n",
    "    state=partitions_state(partitions)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(G, state):\n",
    "    if (len(G.nodes)<=1 or G.number_of_edges()==0): \n",
    "        return 0\n",
    "    partitions = state_partitions(state)\n",
    "    return nx.algorithms.community.modularity(G, partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(G,state):\n",
    "    \"\"\"\n",
    "    how many nodes in a graph are assigned to a community.\n",
    "    \"\"\"\n",
    "    partitions = state_partitions(state)\n",
    "    return nx.algorithms.community.quality.partition_quality(G,partitions)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(G,state):\n",
    "    \"\"\"\n",
    "    measure of how well a community detection algorithm partitions the nodes in \n",
    "    a graph into communities that reflect the underlying structure of the graph.\n",
    "    \"\"\"\n",
    "    partitions = state_partitions(state)\n",
    "    return 1- nx.algorithms.community.quality.partition_quality(G,partitions)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpm_modularity(G,state, gamma=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the modularity score of a graph using the Constant Potts Model (CPM).\n",
    "\n",
    "    Parameters:\n",
    "    - Q (nx.Graph): the graph to calculate the modularity score for.\n",
    "    - gamma (float): the resolution parameter of the CPM.\n",
    "\n",
    "    Returns:\n",
    "    - modularity (float): the modularity score of the graph.\n",
    "    \"\"\"\n",
    "    # Get the adjacency matrix of the graph\n",
    "    A = nx.to_numpy_array(G)\n",
    "    n = len(A)\n",
    "\n",
    "    # Get the total weight of the graph\n",
    "    m = np.sum(A) / 2\n",
    "\n",
    "    # Calculate the degree of each node\n",
    "    ki = np.sum(A, axis=1)\n",
    "\n",
    "    # Calculate the modularity matrix\n",
    "    B = A - gamma * np.outer(ki, ki) / (2 * m)\n",
    "\n",
    "    # Calculate the modularity score\n",
    "    \n",
    "    partitions = state_partitions(state)\n",
    "    ci = np.array(list(partitions))\n",
    "    modularity = 0\n",
    "    for c in ci:\n",
    "        c=np.array(list(c))\n",
    "        modularity += np.sum(B[np.ix_(c, c)])\n",
    "    modularity /= (2 * m)\n",
    "\n",
    "    return modularity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration du voisinage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertex_replacement(G:nx.Graph,state:State)->State:\n",
    "    if(len(set(state.values()))==1):\n",
    "        return state\n",
    "    to_replace = np.random.choice(list(state.keys()))\n",
    "    neighbors = list(G.neighbors(to_replace))\n",
    "    # get neighbors that are not in the same community\n",
    "    neighbors = [n for n in neighbors if state[n] != state[to_replace]]\n",
    "    if len(neighbors)==0:\n",
    "        return state\n",
    "        \n",
    "    replacement = np.random.choice(neighbors)\n",
    "    new_state = state.copy()\n",
    "    new_state[to_replace] = state[replacement]\n",
    "    if modularity(G,new_state) > modularity(G,state):\n",
    "        return new_state\n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deplacement(G,state:State):\n",
    "    if(len(set(state.values()))==1):\n",
    "        return state\n",
    "    # choose random community\n",
    "    community = np.random.choice(list(set(state.values())))\n",
    "    # choose random nodes in community\n",
    "    community_nodes = [node for node in state.keys() if state[node] == community]\n",
    "    number_of_nodes_to_move = np.random.randint(1,len(community_nodes)+1)\n",
    "    nodes_to_move = np.random.choice(community_nodes, number_of_nodes_to_move, replace=False)\n",
    "    # choose random community to move to\n",
    "    new_community = np.random.choice(list(set(state.values()) - set([community])))\n",
    "    new_state = state.copy()\n",
    "    for node in nodes_to_move:\n",
    "        new_state[node] = new_community\n",
    "    if modularity(G,new_state) > modularity(G,state):\n",
    "        return new_state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_fusion(G,state:State):\n",
    "    if(len(set(state.values()))==1):\n",
    "        return state\n",
    "    # choose random community\n",
    "    community = np.random.choice(list(set(state.values())))\n",
    "    # choose random community to fuse with\n",
    "    new_community = np.random.choice(list(set(state.values()) - set([community])))\n",
    "    new_state = state.copy()\n",
    "    for node in new_state.keys():\n",
    "        if new_state[node] == community:\n",
    "            new_state[node] = new_community\n",
    "    if modularity(G,new_state) > modularity(G,state):\n",
    "        return new_state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustif_cross_mutation(G,history:List[Tuple[float,State]],N:int)->List[Tuple[float,State]]:\n",
    "    if len(history) < 2:\n",
    "        return history\n",
    "    # choose random node\n",
    "    node = np.random.choice(list(history[-1][1].keys()))\n",
    "    # get nodes of the same community in every state\n",
    "    neighbor_communities = []\n",
    "    for _,state in history:\n",
    "        comm=state[node]\n",
    "        nodes_to_cross = [node for node in state.keys() if state[node] == comm]\n",
    "        neighbor_communities.append(nodes_to_cross)\n",
    "\n",
    "    states_cross=history.copy()\n",
    "    for i,(_,state) in enumerate(history):\n",
    "        for j,neighbor_community in enumerate(neighbor_communities):\n",
    "            if(i==j):\n",
    "                continue\n",
    "\n",
    "            new_state = state.copy()\n",
    "            for neighbor in neighbor_community:\n",
    "                new_state[neighbor] = history[j][1][node]\n",
    "            # check if new state already exists in history\n",
    "            if(new_state in [state for _,state in states_cross]):\n",
    "                continue\n",
    "            mod=modularity(G,new_state)\n",
    "            candidate = (mod, new_state)\n",
    "            states_cross.append(candidate)\n",
    "    \n",
    "    states_cross=heapq.nlargest(N, states_cross,key=lambda x: x[0])\n",
    "\n",
    "    return states_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_mutation(G:nx.Graph,history:List[Tuple[float,State]],N:int)->List[Tuple[float,State]]:\n",
    "    if len(history) < 2:\n",
    "        return history\n",
    "    # choose random node\n",
    "    states_cross=history.copy()\n",
    "\n",
    "    node = np.random.choice(list(history[-1][1].keys()))\n",
    "    # get two random states in history\n",
    "    i,j = np.random.choice(range(len(history)), 2, replace=False)\n",
    "    if i == j:\n",
    "        return history\n",
    "        \n",
    "    state1=history[i][1]\n",
    "    state2=history[j][1]\n",
    "    neighbors_1 = [n for n in state1 if state1[n] == state1[node]]\n",
    "    neighbors_2 = [n for n in state2 if state2[n] == state2[node]]\n",
    "\n",
    "    new_C2=state2.copy()\n",
    "    for n in neighbors_1:\n",
    "        new_C2[n]=state1[node]\n",
    "\n",
    "    if (not new_C2 in [state for _,state in states_cross]):\n",
    "        mod=modularity(G,new_C2)\n",
    "        candidate = (mod, new_C2)\n",
    "        states_cross.append(candidate)\n",
    "        states_cross=heapq.nlargest(N, states_cross,key=lambda x: x[0])\n",
    "\n",
    "    new_C1=state1.copy()\n",
    "    for n in neighbors_2:\n",
    "        new_C1[n]=state2[node]\n",
    "\n",
    "    if (not new_C1 in [state for _,state in states_cross]):\n",
    "        mod=modularity(G,new_C1)\n",
    "        candidate = (mod, new_C1)\n",
    "        states_cross.append(candidate)\n",
    "        states_cross=heapq.nlargest(N, states_cross,key=lambda x: x[0])\n",
    "\n",
    "    return states_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "voisinage=[vertex_replacement,deplacement,community_fusion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successors(G:nx.Graph,state:State,N:int)->List[Tuple[float,State]]:\n",
    "    successors:List[Tuple[float,State]] = []\n",
    "    for _ in range(N):\n",
    "        new_state=voisinage[np.random.randint(0,3)](G,state)\n",
    "        mod=modularity(G,new_state)\n",
    "        successor=(mod,new_state)\n",
    "        successors.append(successor)\n",
    "        \n",
    "    successors=heapq.nlargest(N, successors,key=lambda x: x[0])\n",
    "    successors=cross_mutation(G,successors,N)\n",
    "    return successors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(G,beam):\n",
    "    mod = [ sol[0] for sol in beam]\n",
    "    mean = sum(mod)/len(beam)\n",
    "    variance = sum((x - mean) ** 2 for x in mod) / len(mod)\n",
    "    return variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "\n",
    "def similarity(beam):\n",
    "    nmi_values = []\n",
    "    n_partitions = len(beam)\n",
    "    states = [ sol[1] for sol in beam]\n",
    "    partitions = [list(state.values()) for state in states]\n",
    "\n",
    "    for i in range(n_partitions):\n",
    "        for j in range(i + 1, n_partitions):\n",
    "            nmi_values.append(nmi(partitions[i], partitions[j]))\n",
    "    return sum(nmi_values) / len(nmi_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch_Data:\n",
    "    \n",
    "    G:nx.Graph\n",
    "    beam:List[Tuple[float,State]]\n",
    "    best_state:State\n",
    "    get_successors:Callable[[nx.Graph,State,int],List[Tuple[float,State]]]\n",
    "    evaluate:Callable[[nx.Graph,State],float]\n",
    "    beam_width:int\n",
    "    nb_successors:int\n",
    "    history:queue.Queue\n",
    "    \n",
    "\n",
    "    def __init__(self, G:nx.Graph,start_state:State, get_successors:Callable[[nx.Graph,State,int],List[Tuple[float,State]]], evaluate:Callable[[nx.Graph,State],float], beam_width:int=10,nb_successors:int=10,hist_size:int=10):\n",
    "        \"\"\"\n",
    "        Effectue une recherche par faisceau (beam search).\n",
    "\n",
    "        Args:\n",
    "            start_state (object): L'état initial de la recherche.\n",
    "            beam_width (int): La taille du faisceau (nombre de candidats à considérer).\n",
    "            max_steps (int): Le nombre maximum d'étapes à effectuer.\n",
    "            get_successors (function): la fonction de voisinage qui retourne les successeurs d'un états (voisins).\n",
    "            evaluate (function): Une fonction qui prend un état en argument et retourne une valeur d'évaluation.\n",
    "\n",
    "        Returns:\n",
    "            object: Le meilleur état trouvé par la beam search.\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.beam_width = beam_width\n",
    "        self.get_successors = get_successors\n",
    "        self.evaluate = evaluate\n",
    "        self.modularity=evaluate(G,start_state)\n",
    "        self.beam = [(self.modularity, start_state)]\n",
    "        self.best_state = start_state\n",
    "        self.nb_successors=nb_successors\n",
    "        self.output=[]\n",
    "        self.hist_size=hist_size\n",
    "        self.history=queue.Queue()\n",
    "        self.history.put(self.modularity)\n",
    "\n",
    "    def search(self,max_steps:int):\n",
    "        for _ in tqdm(range(max_steps)):\n",
    "            candidates:List[Tuple[float,State]] = []  # Liste pour stocker les nouveaux candidats pour chaque étape\n",
    "            candidates.extend(self.beam)\n",
    "            for _, state in self.beam:\n",
    "                successors = self.get_successors(self.G,state,self.nb_successors)\n",
    "                # push successors in candidates\n",
    "                candidates.extend(successors)\n",
    "            self.beam = heapq.nlargest(self.beam_width, candidates,key=lambda x: x[0])  # Sélectionne les meilleurs candidats, la beam c'est l'ensemble des noeuds accepté dans un niveau.\n",
    "            self.history.put(self.beam[0][0])\n",
    "            if(self.history.qsize()>self.hist_size):\n",
    "                self.history.get()\n",
    "            # if all history are the same\n",
    "            if(self.history.qsize()>=self.hist_size and len(set(list(self.history.queue)))==1):\n",
    "                break\n",
    "            \n",
    "            variance=var(self.G,self.beam)\n",
    "            sim=similarity(self.beam)\n",
    "            delta_mod=self.beam[0][0]-self.modularity\n",
    "            nodes_num=G.number_of_nodes()\n",
    "            nb_communities=len(set(list(self.best_state.values())))\n",
    "            data={\"variance\":variance,\"similarity\":sim,\"delta_mod\":delta_mod,\"size\":self.beam_width,\"nodes_num\":nodes_num,\"nb_communities\":nb_communities,\"modularity\":self.beam[0][0]}\n",
    "            self.output.append(data)\n",
    "\n",
    "            if self.modularity < self.beam[0][0]:\n",
    "                self.best_state = self.beam[0][1]  # Meilleur état trouvé\n",
    "                self.modularity = self.beam[0][0]\n",
    "        \n",
    "        return self.best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(data):\n",
    "    return nx.Graph(data.edge_index.t().tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Generate Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=[]\n",
    "from torch_geometric.datasets import TUDataset\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='MUTAG')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='IMDB-BINARY')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='IMDB-MULTI')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='REDDIT-BINARY')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='REDDIT-MULTI-5K')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='COLLAB')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='PROTEINS')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='DD')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='NCI1')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='NCI109')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='ENZYMES')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='PTC_MR')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='NCI1')[0])\n",
    "datasets.append(TUDataset(root='data/TUDataset', name='NCI109')[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=[]\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "datasets.append(GNNBenchmarkDataset(root='data/GNNBenchmarkDataset', name='MNIST')[0])\n",
    "datasets.append(GNNBenchmarkDataset(root='data/GNNBenchmarkDataset', name='CIFAR10')[0])\n",
    "datasets.append(GNNBenchmarkDataset(root='data/GNNBenchmarkDataset', name='PATTERN')[0])\n",
    "datasets.append(GNNBenchmarkDataset(root='data/GNNBenchmarkDataset', name='CLUSTER')[0])\n",
    "from torch_geometric.datasets import Planetoid\n",
    "datasets.append(Planetoid(root='data/Planetoid', name='Cora')[0])\n",
    "datasets.append(Planetoid(root='data/Planetoid', name='CiteSeer')[0])\n",
    "from torch_geometric.datasets import QM9\n",
    "datasets.append(QM9(root='data/QM9')[0])\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "datasets.append(MNISTSuperpixels(root='data/MNISTSuperpixels')[0])\n",
    "from torch_geometric.datasets import FakeDataset\n",
    "datasets.append(FakeDataset()[0])\n",
    "from torch_geometric.datasets import CitationFull\n",
    "datasets.append(CitationFull(root='data/CitationFull', name='citeseer')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 24/300 [00:05<00:58,  4.72it/s]\n",
      " 10%|▉         | 29/300 [00:08<01:15,  3.60it/s]\n",
      "  4%|▍         | 13/300 [00:08<03:20,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "DATASET=[]\n",
    "beam_size=10\n",
    "while beam_size<30:\n",
    "    print(beam_size)\n",
    "    for dataset in datasets:\n",
    "        G=get_graph(dataset)\n",
    "        # initial_state=init_alea(G)\n",
    "        initial_state=init_greedy(G)\n",
    "        beam_data=BeamSearch_Data(G,start_state=initial_state,get_successors=get_successors,evaluate=modularity,beam_width=beam_size,nb_successors=10)\n",
    "        beam_data.search(max_steps=300)\n",
    "        DATASET+=beam_data.output\n",
    "    beam_size+=5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset\n",
    "\n",
    "name file with the next format\n",
    "    \n",
    "```\n",
    "beam_search_[TUDataset/Other]_[alea/greedy]_[beamSize].csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(DATASET)\n",
    "df.to_csv(\"beam_search_Other_greedy_10_15_25.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
